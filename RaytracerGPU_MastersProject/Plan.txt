Why am I doing this? What am I solving?

In order to make runtime raytracing feasible, it's necessary to optimize some of the more expensive
components of a monte-carlo raytracer. The inclusion of a BVH (bounding volume hierarchy) reduces
the runtime cost of hit detection, however, the cost of building the BVH and transferring it to the gpu
is expensive. To balance realistic rendering techniques with runtime performance, this paper aims
to implement a method of building the BVH on the gpu described by {citation}.
(perhaps add in that, alongside an implementation, comparisons can also be made (though don't wanna give myself more work than is necessary)) 

Constructing a GPU-based Real-time Raytracer which assembles and uses a Bounding Volume Hierarchy in
a Vulkan Compute Shader. Raytraces using a Vulkan Compute Shader, then draws to screen using
the Vulkan Graphics pipeline, resulting in a raytraced image.

Step 1: (Done)
    - make a vulkan compute shader do something and produce output
        - ill do the generation of a bifurcation plot of the logistic map

Step 1.5: (optional)
    - create 3D space with camera and keyboard movement to view graph in
        - walking close to points makes them invisible (or discarded) till distance increases
        - could continue to compute and put increases in compute iterations behind prev iterations
        - would mean turning logistic (x, r) pairs into vertices probably

Step 2:
    - setup the compute shader to calculate color values for a scene using gpu-based raytracing
        - will use some previous work done in my Raytracer project
        - no bvh will be used here
        - still not sure what the use of the graphics pipeline is if the image is just the raytraced result
            - perhaps im thinking the raytracing will achieve more than it actually will?

Step 3:
    - create a bvh on the cpu and sent it to the gpu to improve raytracing speed and framerate
        - should be able to store it inside an Shader Storage Buffer Object or even a Uniform Buffer since the size is known
        - not sure how to maintain that pointer-based relationship in the glsl shader language

Step 4:
    - move bvh construction onto the the gpu, perhaps in it's own compute shader before the raytracing compute shader
        - will be following the idea from a paper on this one
            - title: Fast Parallel Bounding Volume Hierarchy Construction
	            authors: Xiaozi Guo, Juan Zhang, Mingquan Zhou
	            publication date: April 16, 2020
	            doi: 10.1109/IPEC49694.2020.9115145

Stretch Goals/Extra Goals:
- add post processing steps (ie. image -> image transformations after the graphics pipeline runs)
- asynchronous compute shaders


TODO:
//- setup sending multiple ray casts per pixel in one image
- add ONB (orthonormal basis) class/utilities and BRDF
- BVH & other optimizations
- use swapchain right to avoid stalling gpu work while updating gameobjects, waiting for image clear, waiting for modelToWorld, waiting for raytrace
- setup proper camera class for raytracing (and put values from it into ubo)
    - lookAt, lookFrom, upDir, facingDir
- re-examine gameobject api
- re-examine RTModel api
- fix bug where all scenes require non-zero length lists of triangles and spheres